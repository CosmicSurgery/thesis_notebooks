{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3297a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model parameters\n",
    "'''\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "M = 2 # Number of Spiking motifs\n",
    "N = 10 # Number of input neurons\n",
    "D = 31 # temporal depth of receptive field\n",
    "T = 1000\n",
    "dt = 1\n",
    "nrn_fr = 40 # hz\n",
    "pg_fr = 6 # hz\n",
    "background_noise_fr = 5 # h\n",
    "\n",
    "np.random.seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75bb7417",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Setup\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "disp_figs = True\n",
    "import colorsys\n",
    "\n",
    "def create_color_spectrum(num_labels):\n",
    "    golden_ratio_conjugate = 0.618033988749895\n",
    "    hues = np.arange(num_labels)\n",
    "    hues = (hues * golden_ratio_conjugate) % 1.0\n",
    "    saturations = np.ones(num_labels) * 0.8\n",
    "    lightness = np.ones(num_labels) * 0.6\n",
    "\n",
    "    # Convert HSL to RGB and then to hexadecimal\n",
    "    colors = []\n",
    "    for h, s, l in zip(hues, saturations, lightness):\n",
    "        r, g, b = [int(255 * x) for x in colorsys.hls_to_rgb(h, l, s)]\n",
    "        colors.append(f'#{r:02x}{g:02x}{b:02x}')\n",
    "\n",
    "    return colors\n",
    "\n",
    "# Existing colors represented as hexadecimal strings\n",
    "existing_colors = np.array(['#000000','#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "                            '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n",
    "\n",
    "# Create a palette with 101 colors (11 existing + 90 new)\n",
    "num_new_colors = 90\n",
    "new_colors = create_color_spectrum(num_new_colors)\n",
    "palette = np.concatenate([existing_colors, new_colors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5417153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1031)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Synthetic Data Generation\n",
    "'''\n",
    "# Dense K: matrix of binary images of sizeNxDxM\n",
    "# Sparse K: set of (delay d, neuron a, and pg b)\n",
    "\n",
    "K_dense = np.random.randint(0,999,(N,D,M))\n",
    "K_dense[K_dense < nrn_fr] = 1\n",
    "K_dense[K_dense >= nrn_fr] = 0\n",
    "K_sparse = np.where(K_dense)\n",
    "K_sparse = (K_sparse[0],K_sparse[1],K_sparse[2]+1)\n",
    "\n",
    "# dense B: the binary image of the occurrences of the spiking motif as a ( M x T) matrix\n",
    "# spare B: set of all times t and pg's b\n",
    "B_dense = np.random.randint(0,999,(M,T))\n",
    "B_dense[B_dense < pg_fr] = 1\n",
    "B_dense[B_dense >= pg_fr] = 0\n",
    "B_sparse = np.where(B_dense)\n",
    "B_sparse = (B_sparse[0]+1,B_sparse[1])# This way the first motif starts at index 1 instead of index 0\n",
    "\n",
    "# now to make the full raster plot keeping the labels in-tact\n",
    "# dense A: the layered binary images of all neuron spikes by PG ( N x T x M\n",
    "A_dense = np.zeros((N,T+D,M+1))\n",
    "A_dense[...,0] = np.random.randint(0,999,(N,T+D))\n",
    "A_dense[...,0] = (A_dense[...,0] < background_noise_fr).astype('int')\n",
    "for i in range(len(B_sparse[0])):\n",
    "    t = B_sparse[1][i]\n",
    "    b = B_sparse[0][i]\n",
    "    A_dense[:, t:t+D, b] += K_dense[...,b-1]\n",
    "    \n",
    "A_dense.shape\n",
    "# A_dense = A_dense[:,:1000,:]\n",
    "A_sparse = np.where(A_dense)\n",
    "debug = A_dense\n",
    "A_dense = np.sum(A_dense,axis=2)\n",
    "A_dense[A_dense>1] = 1\n",
    "A_dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c99c31b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189, 31, 485, 115]: 2 occurrences\n",
      "[189, 516, 115]: 2 occurrences\n",
      "[636]: 1 occurrences\n",
      "[92]: 1 occurrences\n",
      "[0]: 1 occurrences\n",
      "[218, 2, 485, 115]: 1 occurrences\n",
      "[163]: 1 occurrences\n",
      "[189, 29, 487]: 1 occurrences\n",
      "[132, 153, 45, 218, 183, 82]: 1 occurrences\n",
      "[38, 132, 153, 45, 218, 183, 82]: 7 occurrences\n",
      "[189, 29, 2, 485, 115]: 9 occurrences\n",
      "[218, 2, 175, 310, 115]: 1 occurrences\n",
      "[701]: 1 occurrences\n",
      "[153, 45, 218, 265]: 1 occurrences\n",
      "[170, 153, 45, 218, 183, 82]: 1 occurrences\n",
      "[38, 132, 153, 45, 218, 183]: 2 occurrences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_12444\\2770237764.py:35: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  HDPs.append(linkage(dissimilarity, method='complete'))\n",
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_12444\\2770237764.py:36: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  clusters += list((n*100)+fcluster(linkage(dissimilarity, method='complete'), cutoff, criterion='distance'))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Post-processing and clustering\n",
    "'''\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Windowing the data\n",
    "window_dim = (int(np.ceil(D/2)), N) # choosing a window to surround the spikes, this is a prediction but for now we will use the a priori depth D\n",
    "valid_spikes = (A_sparse[1] > window_dim[0]) & (A_sparse[1] < T- window_dim[0]) # spikes closer to border will not be counted for simplicity\n",
    "A_trunc = (A_sparse[0][valid_spikes],A_sparse[1][valid_spikes].copy(),A_sparse[2][valid_spikes].copy()) # creating a new \"A_sparse\" with only valid spikes\n",
    "M_labels = A_sparse[2][valid_spikes]\n",
    "N_labels = A_sparse[0][valid_spikes]\n",
    "T_labels = A_sparse[1][valid_spikes]\n",
    "windows = np.zeros((len(T_labels), N, window_dim[0]*2)) # creating a matrix of all of the windows surrounding every spike in the matrix\n",
    "for i,window_time in enumerate(T_labels):\n",
    "    windows[i,...] = A_dense[:,-window_dim[0]+window_time:window_dim[0]+window_time]\n",
    "    \n",
    "HDPs = []\n",
    "sim_mats = []\n",
    "# Set the cutoff value for clustering\n",
    "cutoff = 1\n",
    "\n",
    "# Get the cluster assignments for each spike based on the hierarchical clustering\n",
    "clusters = []\n",
    "for n in range(N):\n",
    "    test = windows[N_labels==n]\n",
    "    if len(test) > 0:\n",
    "        x = np.zeros((len(test),len(test)))\n",
    "        for i in range(test.shape[0]):\n",
    "            for j in range(test.shape[0]):\n",
    "                x[i,j] = np.sum(test[i]*test[j])/ min(np.sum(test[i]),np.sum(test[j]))\n",
    "        sim_mats.append(x)\n",
    "        dissimilarity = 1 - x\n",
    "        HDPs.append(linkage(dissimilarity, method='complete'))\n",
    "        clusters += list((n*100)+fcluster(linkage(dissimilarity, method='complete'), cutoff, criterion='distance'))\n",
    "\n",
    "clusters= np.array(clusters)\n",
    "\n",
    "time_differences = []\n",
    "cluster_sq = {}\n",
    "for cluster in np.unique(clusters):\n",
    "    time_differences.append(list(np.diff(A_trunc[1][clusters == cluster])))\n",
    "    if str(list(np.diff(A_trunc[1][clusters == cluster]))) in cluster_sq.keys():\n",
    "        cluster_sq[str(list(np.diff(A_trunc[1][clusters == cluster])))] = cluster_sq[str(list(np.diff(A_trunc[1][clusters == cluster])))] + [cluster]\n",
    "    else: \n",
    "        cluster_sq[str(list(np.diff(A_trunc[1][clusters == cluster])))] = [cluster]\n",
    "        \n",
    "''' \n",
    "This is the second round of clustering. Only patterns that repeat across multiple neurons are considered a motif. \n",
    "\n",
    "\n",
    "with some help from chatgpt\n",
    "'''\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Convert the list of lists to a set of tuples to remove duplicates\n",
    "unique_sublists_set = set(tuple(sublist) for sublist in time_differences if sublist)\n",
    "\n",
    "# Convert the set of tuples back to a list of lists\n",
    "unique_sublists = [list(sublist) for sublist in unique_sublists_set]\n",
    "\n",
    "# Count the occurrences of each unique sublist in the original list\n",
    "sublist_counts = Counter(tuple(sublist) for sublist in time_differences if sublist)\n",
    "\n",
    "# Print the unique sublists and their respective counts\n",
    "for sublist in unique_sublists:\n",
    "    count = sublist_counts[tuple(sublist)]\n",
    "    print(f\"{sublist}: {count} occurrences\")\n",
    "\n",
    "sublist_keys_np = np.array([list(key) for key in sublist_counts.keys()],dtype='object')\n",
    "sublist_keys_filt = sublist_keys_np[np.array(list(sublist_counts.values())) >1] # only bother clustering repetitions that appear for more than one neuron\n",
    "\n",
    "''' to visualize the clusters'''\n",
    "\n",
    "recovered_labels = np.zeros_like(clusters)\n",
    "for l, key in enumerate(sublist_keys_filt):\n",
    "    for k in cluster_sq[str(key)]:\n",
    "        recovered_labels[clusters == k] = l+1\n",
    "\n",
    "''' to get the timings'''\n",
    "\n",
    "# Sort y according to x\n",
    "sorted_indices = np.argsort(A_trunc[1])\n",
    "sorted_x = A_trunc[1][sorted_indices]\n",
    "\n",
    "all_times = []\n",
    "all_labels = []\n",
    "for key in sublist_keys_filt:\n",
    "    pattern_repetition_labels = np.zeros((len(cluster_sq[str(key)]),len(clusters)))\n",
    "    for i,k in enumerate(cluster_sq[str(key)]):\n",
    "        pattern_repetition_labels[i][clusters==k] = 1\n",
    "        pattern_repetition_labels[i] *= np.cumsum(pattern_repetition_labels[i])\n",
    "    pattern_repetition_labels = np.sum(pattern_repetition_labels,axis=0,dtype='int')\n",
    "    all_labels.append(pattern_repetition_labels)\n",
    "    \n",
    "    sorted_y = pattern_repetition_labels[sorted_indices]\n",
    "    pattern_times = np.array([sorted_x[sorted_y==i][0] for i in range(1,max(pattern_repetition_labels)+1)])\n",
    "    all_times.append(pattern_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96fedc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mapping Cluster Labels for Visualization\n",
    "'''\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "labels_array_1 = recovered_labels\n",
    "labels_array_2 = A_trunc[2]\n",
    "\n",
    "# Get unique labels from both arrays\n",
    "unique_labels_1 = np.unique(labels_array_1)\n",
    "unique_labels_2 = np.unique(labels_array_2)\n",
    "\n",
    "# Create a cost matrix where each entry represents the cost of matching two labels\n",
    "cost_matrix = np.zeros((len(unique_labels_1), len(unique_labels_2)))\n",
    "\n",
    "for i, label_1 in enumerate(unique_labels_1):\n",
    "    for j, label_2 in enumerate(unique_labels_2):\n",
    "        common_elements = np.logical_and(labels_array_1 == label_1, labels_array_2 == label_2)\n",
    "        cost_matrix[i, j] = -np.sum(common_elements)\n",
    "\n",
    "# Use the Hungarian algorithm to find the optimal mapping\n",
    "row_indices, col_indices = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "# Create a dictionary to store the many-to-one mapping\n",
    "mapping_dict = {}\n",
    "\n",
    "# Populate the mapping dictionary with the optimal mapping\n",
    "for i, j in zip(row_indices, col_indices):\n",
    "    label_1 = unique_labels_1[i]\n",
    "    label_2 = unique_labels_2[j]\n",
    "    \n",
    "    if label_1 in mapping_dict:\n",
    "        mapping_dict[label_1].append(label_2)\n",
    "    else:\n",
    "        mapping_dict[label_1] = [label_2]\n",
    "\n",
    "# Find unmatched labels from labels_array_1\n",
    "unmatched_labels_1 = np.setdiff1d(unique_labels_1, list(mapping_dict.keys()))\n",
    "\n",
    "# Handle unmatched labels by assigning them to the closest label in labels_array_2\n",
    "for label_1 in unmatched_labels_1:\n",
    "    closest_label_2 = unique_labels_2[np.argmin(cost_matrix[label_1, :])]\n",
    "    mapping_dict[label_1] = [closest_label_2]\n",
    "\n",
    "# Map the labels from labels_array_1 to labels_array_2 using the mapping dictionary\n",
    "# Use the get method with a default value of -1 for labels without a mapping\n",
    "mapped_labels = np.array([mapping_dict[label] for label in labels_array_1]).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49651f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8394160583941606, 0.8584615384615385, 137, 325)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Performance\n",
    "'''\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "total_TP = 0\n",
    "total_TN = 0\n",
    "for m in range(M):\n",
    "    TP += np.sum(((A_trunc[2]==m) & (mapped_labels==m)) == True)\n",
    "    TN += np.sum(((A_trunc[2]!=m) & (mapped_labels!=m)) == True)\n",
    "    total_TP += np.sum(A_trunc[2]==m)\n",
    "    total_TN += np.sum(A_trunc[2]!=m)\n",
    "    \n",
    "print(TP/total_TP, TN/total_TN, total_TP,total_TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71956fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
