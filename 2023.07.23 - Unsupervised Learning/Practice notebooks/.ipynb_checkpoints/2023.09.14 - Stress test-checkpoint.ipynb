{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "8a2fbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from collections import Counter\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "6787db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'scan_stats.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "9be94178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(params):\n",
    "    '''\n",
    "    Synthetic Data Generation\n",
    "    '''\n",
    "    # Dense K: matrix of binary images of sizeNxDxM\n",
    "    # Sparse K: set of (delay d, neuron a, and pg b)\n",
    "\n",
    "    M,N,D,T,nrn_fr,pg_fr,background_noise_fr = params['M'], params['N'], params['D'], params['T'], params['nrn_fr'],params['pg_fr'],params['background_noise_fr'],    \n",
    "    '''\n",
    "    Synthetic Data Generation\n",
    "    '''\n",
    "    # Dense K: matrix of binary images of sizeNxDxM\n",
    "    # Sparse K: set of (delay d, neuron a, and pg b)\n",
    "\n",
    "    K_dense = np.random.rand(N,D,M)*1000\n",
    "    nrn_frs = np.zeros((M))\n",
    "    for m in range(M):\n",
    "        nrn_frs[m] = np.random.poisson(nrn_fr)\n",
    "        K_dense[:,:,m] = (K_dense[:,:,m] < nrn_frs[m]).astype('int')\n",
    "    K_sparse = np.where(K_dense)\n",
    "    K_sparse = (K_sparse[0],K_sparse[1],K_sparse[2]+1)\n",
    "\n",
    "\n",
    "    # dense B: the binary image of the occurrences of the spiking motif as a ( M x T) matrix\n",
    "    # spare B: set of all times t and pg's b\n",
    "    B_dense = np.random.rand(M,T)*1000\n",
    "    pg_frs = np.zeros((M))\n",
    "    for m in range(M):\n",
    "        pg_frs[m] = np.random.poisson(pg_fr)\n",
    "        B_dense[m,:] = (B_dense[m,:] < pg_frs[m]).astype('int')\n",
    "    B_sparse = np.where(B_dense)\n",
    "    B_sparse = (B_sparse[0]+1,B_sparse[1])# This way the first motif starts at index 1 instead of index 0\n",
    "\n",
    "    # now to make the full raster plot keeping the labels in-tact\n",
    "    # dense A: the layered binary images of all neuron spikes by PG ( N x T x M\n",
    "    A_dense = np.zeros((N,T+D,M+1))\n",
    "    A_dense[...,0] = np.random.rand(N,T+D)*1000\n",
    "    A_dense[...,0] = (A_dense[...,0] < background_noise_fr).astype('int')\n",
    "    for i in range(len(B_sparse[0])):\n",
    "        t = B_sparse[1][i]\n",
    "        b = B_sparse[0][i]\n",
    "        A_dense[:, t:t+D, b] += K_dense[...,b-1]\n",
    "\n",
    "    A_sparse = np.where(A_dense)\n",
    "    A_dense = np.sum(A_dense,axis=2)\n",
    "    A_dense[A_dense>1] = 1\n",
    "    \n",
    "    stats = _get_stats(A_sparse, B_sparse, K_sparse)\n",
    "    \n",
    "    return A_dense, A_sparse, B_dense, B_sparse, K_dense, K_sparse, stats\n",
    "\n",
    "def _get_stats(A_sparse, B_sparse, K_sparse):\n",
    "    A_fr = [A_sparse[0].tolist().count(n) for n in np.unique(A_sparse[0])]\n",
    "    B_fr = [B_sparse[0].tolist().count(n) for n in np.unique(B_sparse[0])]\n",
    "    K_fr = [[K_sparse[0][K_sparse[2]==m].tolist().count(n) for n in np.unique(A_sparse[0])] for m in np.unique(K_sparse[2])]\n",
    "    return A_fr, B_fr, K_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "867f603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_raster(T_labels, N_labels, window_dim = None):\n",
    "    '''\n",
    "    T_labels an array of spiketimes\n",
    "    N_labels corresponding array of neuron labels\n",
    "    window_dim is the size of the window to cluster the spikes\n",
    "    '''\n",
    "    if window_dim == None:\n",
    "        window_dim = 100\n",
    "        \n",
    "    T_labels = np.round(T_labels).astype(int)\n",
    "    T_labels, N_labels = np.unique(np.array([T_labels,N_labels]),axis=1) # This removes any spikes that occur at the same neuron at the same time\n",
    "    N=max(N_labels)+1\n",
    "\n",
    "    print(f'{len(T_labels)} Windows')\n",
    "    windows = np.zeros((len(T_labels)),dtype='object')\n",
    "    for i,window_time in enumerate(T_labels):\n",
    "        condition = (T_labels > window_time-window_dim) & (T_labels < window_time + window_dim)\n",
    "        window = np.array([T_labels[condition]-window_time, N_labels[condition]]).T\n",
    "        window =  {tuple(row) for row in  window}\n",
    "        windows[i] = window\n",
    "        print(f'Windowing... {round(100*i/len(T_labels))}%',end='\\r')\n",
    "        \n",
    "   \n",
    "    max_iter = 50\n",
    "    x = np.arange(0,100)\n",
    "    fun = lambda x,c : x**(1+c)/10**(2*c+1)\n",
    "    list_cutoffs = 0.01 + np.concatenate(((np.arange(0,100)/100)[:15], np.round(fun(x,.8),2)[10:25],np.round(fun(x,.8),2)[25::4],[10]))\n",
    "\n",
    "    opt_cutoff = 0\n",
    "    max_seq_rep = 0\n",
    "    sim_mats = _get_sim_mats(windows, T_labels, N_labels)\n",
    "    \n",
    "    \n",
    "    print(\"Clustering...\",end=\"\\r\")\n",
    "\n",
    "    for i, cutoff in enumerate(list_cutoffs): \n",
    "        clusters = _cluster_windows(cutoff, N_labels, sim_mats)\n",
    "        cluster_sq, _sq_counts, sublist_keys_filt = _check_seq(clusters, T_labels, N_labels)\n",
    "\n",
    "        if len(sublist_keys_filt) != 0:\n",
    "            max_ = np.max([len(k) for k in sublist_keys_filt])\n",
    "            if max_seq_rep < max_:\n",
    "                max_seq_rep = max_\n",
    "                opt_cutoff=cutoff\n",
    "\n",
    "        print(f'progress - {100*i/max_iter}% | cutoff - {cutoff} | opt_cutoff - {opt_cutoff} | most_detections - {max_seq_rep}',end='\\r')\n",
    "\n",
    "    clusters = _cluster_windows(opt_cutoff, N_labels, sim_mats)\n",
    "    cluster_sq, sq_counts, sublist_keys_filt = _check_seq(clusters, T_labels, N_labels)\n",
    "    \n",
    "\n",
    "    ''' to get the timings'''\n",
    "\n",
    "    # Sort y according to x\n",
    "    sorted_indices = np.argsort(T_labels)\n",
    "    sorted_x = T_labels[sorted_indices]\n",
    "    \n",
    "    print(\"Re-Clustering...\",end=\"\\r\")\n",
    "\n",
    "    all_times = []\n",
    "    all_labels = []\n",
    "    for key in sublist_keys_filt:\n",
    "        pattern_repetition_labels = np.zeros((len(cluster_sq[str(key)]),len(clusters)))\n",
    "        for i,k in enumerate(cluster_sq[str(key)]):\n",
    "            pattern_repetition_labels[i][clusters==k] = 1\n",
    "            pattern_repetition_labels[i] *= np.cumsum(pattern_repetition_labels[i])\n",
    "        pattern_repetition_labels = np.sum(pattern_repetition_labels,axis=0,dtype='int')\n",
    "        all_labels.append(pattern_repetition_labels)\n",
    "\n",
    "        sorted_y = pattern_repetition_labels[sorted_indices]\n",
    "        pattern_times = np.array([sorted_x[sorted_y==i][0] for i in range(1,max(pattern_repetition_labels)+1)])\n",
    "        all_times.append(pattern_times)\n",
    "        \n",
    "    print(\"Extracting templates...\",end=\"\\r\")\n",
    "\n",
    "    pattern_template = []\n",
    "    patterns = []\n",
    "    for i in range(len(all_times)):\n",
    "        pattern = []\n",
    "        pattern_template.append([])\n",
    "        for time in all_times[i]:\n",
    "            condition = (T_labels > time-window_dim*2) & (T_labels < time + window_dim*2)\n",
    "            pattern = [tuple(k) for k in np.array([T_labels[condition]-time, N_labels[condition]]).T] # creating a list of tuples\n",
    "            pattern_template[-1] += pattern # adds all points of each pattern to template_pattern\n",
    "            patterns.append(pattern)\n",
    "        print(f\"Extracting templates... {round(100*i/len(all_times))}%\",end=\"\\r\")\n",
    "    \n",
    "    print(f\"{len(pattern_template)} patterns found...\")\n",
    "\n",
    "    for i,pattern in enumerate(pattern_template):\n",
    "        counts = [pattern.count(k) for k in pattern]\n",
    "        pattern_template[i] = np.array(pattern)[np.where(counts == np.max(counts))[0]]\n",
    "        pattern_template[i][:,0] -= min(pattern_template[i][:,0])\n",
    "        pattern_template[i] = np.unique(pattern_template[i],axis=0)\n",
    "    \n",
    "    if len(pattern_template) == 0:\n",
    "        return pattern_template, sublist_keys_filt, None\n",
    "    \n",
    "    win_size = (K_dense.shape[0],1+max([max(k[:,0]) for k in pattern_template]))\n",
    "    pattern_img = np.zeros((len(pattern_template),*win_size))\n",
    "    for p,pattern in enumerate(pattern_template):\n",
    "        for (i,j) in pattern:\n",
    "            pattern_img[p,j,i] = 1\n",
    "\n",
    "    return pattern_template, sublist_keys_filt, pattern_img\n",
    "\n",
    "def _get_sim_mats(windows, T_labels, N_labels):\n",
    "    sim_mats = np.zeros(np.max(N_labels)+1,dtype='object')\n",
    "    for n in np.unique(N_labels):\n",
    "        idc = np.where(N_labels==n)[0]\n",
    "        windows_n = windows[idc]\n",
    "        if len(windows_n) > 1:\n",
    "            x = np.zeros((len(windows_n),len(windows_n)))\n",
    "            for i in range(windows_n.shape[0]):\n",
    "                for j in range(windows_n.shape[0]):\n",
    "                    common_rows = windows_n[i].intersection(windows_n[j])\n",
    "                    num_identical_rows = len(common_rows)\n",
    "                    x[i,j] = len(common_rows)/min(len(windows_n[i]),len(windows_n[j]))\n",
    "            np.fill_diagonal(x,0)# make sure the diagonals are zero, this is important the more spikes there are...\n",
    "            sim_mats[n] = x-1 \n",
    "        print(f\"Generating sim matrices {round(n*100/np.max(N_labels))}%\", end = \"\\r\")\n",
    "    return sim_mats\n",
    "\n",
    "def _cluster_windows(cutoff, N_labels, sim_mats):\n",
    "    clusters = np.zeros_like(N_labels)\n",
    "    for n in np.unique(N_labels):\n",
    "        idc = np.where(N_labels==n)[0]\n",
    "        if (type(sim_mats[n]) == np.ndarray) and (not np.all(sim_mats[n] == 0)):\n",
    "            l = max(clusters)+1\n",
    "            clusters[idc]= l+fcluster(linkage(sim_mats[n], method='complete'), cutoff, criterion='distance')\n",
    "    return clusters\n",
    "\n",
    "def _check_seq(clusters, T_labels, N_labels):\n",
    "\n",
    "    time_differences = []\n",
    "    cluster_sq = {}\n",
    "    for cluster in np.unique(clusters):\n",
    "        temp = list(np.diff(np.unique(T_labels[clusters == cluster])))\n",
    "        str_temp = str(temp)\n",
    "        time_differences.append(temp)\n",
    "        if str_temp in cluster_sq.keys():\n",
    "            cluster_sq[str_temp] = cluster_sq[str_temp] + [cluster]\n",
    "        else:\n",
    "            cluster_sq[str_temp] = [cluster]\n",
    "\n",
    "    # Convert the list of lists to a set of tuples to remove duplicates\n",
    "    unique_sublists_set = set(tuple(sublist) for sublist in time_differences if sublist)\n",
    "\n",
    "    # Convert the set of tuples back to a list of lists\n",
    "    unique_sublists = [list(sublist) for sublist in unique_sublists_set]\n",
    "\n",
    "    # Count the occurrences of each unique sublist in the original list\n",
    "    sublist_counts = Counter(tuple(sublist) for sublist in time_differences if sublist)\n",
    "\n",
    "    # Print the unique sublists and their respective counts\n",
    "    sq_counts = np.zeros(len(sublist_counts)) \n",
    "    for i,sublist in enumerate(unique_sublists):\n",
    "        count = sublist_counts[tuple(sublist)]\n",
    "        sq_counts[i] = count\n",
    "    #     print(f\"{sublist}: {count} occurrences\")\n",
    "    sublist_keys_np = np.array([list(key) for key in sublist_counts.keys()],dtype='object')\n",
    "    sublist_keys_filt = sublist_keys_np[np.array(list(sublist_counts.values())) >1] # only bother clustering repetitions that appear for more than one neuron\n",
    "    \n",
    "    return cluster_sq, sq_counts, sublist_keys_filt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "ccf37a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import correlate\n",
    "def get_acc(ground_truths,detected_patterns):\n",
    "    # Calculate cross-correlation matrix\n",
    "    cross_corr_matrix = np.zeros((ground_truths.shape[2], detected_patterns.shape[2]))\n",
    "    SM_acc = np.zeros((ground_truths.shape[2]))\n",
    "    \n",
    "    if len(detected_patterns) == 0:\n",
    "        return SM_acc, cross_corr_matrix\n",
    "    \n",
    "    for ground_truths_idx in range(ground_truths.shape[2]):\n",
    "        for detected_patterns_idx in range(detected_patterns.shape[2]):\n",
    "            cross_corr = np.zeros((ground_truths.shape[1]+detected_patterns.shape[1]-1))\n",
    "            for n in range(ground_truths.shape[0]):\n",
    "                cross_corr += correlate(ground_truths[n, :, ground_truths_idx], detected_patterns[n, :, detected_patterns_idx], mode='full')\n",
    "            max_corr = np.max(cross_corr) / max(np.sum(ground_truths[...,ground_truths_idx]),np.sum(detected_patterns[...,detected_patterns_idx]))\n",
    "            cross_corr_matrix[ground_truths_idx, detected_patterns_idx] = max_corr\n",
    "#     print(cross_corr_matrix)\n",
    "#     print( np.sum(ground_truths[...,ground_truths_idx]))\n",
    "    SM_acc = np.max(cross_corr_matrix,axis=1)\n",
    "    return SM_acc, cross_corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "2066d78a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'M': 1, 'N': 100, 'D': 30, 'T': 1000, 'nrn_fr': 5, 'pg_fr': 10, 'background_noise_fr': 0}\n",
      "Generating raster plot...\n",
      "168 Windows\n",
      "Windowing... 0%\r",
      "Windowing... 1%\r",
      "Windowing... 1%\r",
      "Windowing... 2%\r",
      "Windowing... 2%\r",
      "Windowing... 3%\r",
      "Windowing... 4%\r",
      "Windowing... 4%\r",
      "Windowing... 5%\r",
      "Windowing... 5%\r",
      "Windowing... 6%\r",
      "Windowing... 7%\r",
      "Windowing... 7%\r",
      "Windowing... 8%\r",
      "Windowing... 8%\r",
      "Windowing... 9%\r",
      "Windowing... 10%\r",
      "Windowing... 10%\r",
      "Windowing... 11%\r",
      "Windowing... 11%\r",
      "Windowing... 12%\r",
      "Windowing... 12%\r",
      "Windowing... 13%\r",
      "Windowing... 14%\r",
      "Windowing... 14%\r",
      "Windowing... 15%\r",
      "Windowing... 15%\r",
      "Windowing... 16%\r",
      "Windowing... 17%\r",
      "Windowing... 17%\r",
      "Windowing... 18%\r",
      "Windowing... 18%\r",
      "Windowing... 19%\r",
      "Windowing... 20%\r",
      "Windowing... 20%\r",
      "Windowing... 21%\r",
      "Windowing... 21%\r",
      "Windowing... 22%\r",
      "Windowing... 23%\r",
      "Windowing... 23%\r",
      "Windowing... 24%\r",
      "Windowing... 24%\r",
      "Windowing... 25%\r",
      "Windowing... 26%\r",
      "Windowing... 26%\r",
      "Windowing... 27%\r",
      "Windowing... 27%\r",
      "Windowing... 28%\r",
      "Windowing... 29%\r",
      "Windowing... 29%\r",
      "Windowing... 30%\r",
      "Windowing... 30%\r",
      "Windowing... 31%\r",
      "Windowing... 32%\r",
      "Windowing... 32%\r",
      "Windowing... 33%\r",
      "Windowing... 33%\r",
      "Windowing... 34%\r",
      "Windowing... 35%\r",
      "Windowing... 35%\r",
      "Windowing... 36%\r",
      "Windowing... 36%\r",
      "Windowing... 37%\r",
      "Windowing... 38%\r",
      "Windowing... 38%\r",
      "Windowing... 39%\r",
      "Windowing... 39%\r",
      "Windowing... 40%\r",
      "Windowing... 40%\r",
      "Windowing... 41%\r",
      "Windowing... 42%\r",
      "Windowing... 42%\r",
      "Windowing... 43%\r",
      "Windowing... 43%\r",
      "Windowing... 44%\r",
      "Windowing... 45%\r",
      "Windowing... 45%\r",
      "Windowing... 46%\r",
      "Windowing... 46%\r",
      "Windowing... 47%\r",
      "Windowing... 48%\r",
      "Windowing... 48%\r",
      "Windowing... 49%\r",
      "Windowing... 49%\r",
      "Windowing... 50%\r",
      "Windowing... 51%\r",
      "Windowing... 51%\r",
      "Windowing... 52%\r",
      "Windowing... 52%\r",
      "Windowing... 53%\r",
      "Windowing... 54%\r",
      "Windowing... 54%\r",
      "Windowing... 55%\r",
      "Windowing... 55%\r",
      "Windowing... 56%\r",
      "Windowing... 57%\r",
      "Windowing... 57%\r",
      "Windowing... 58%\r",
      "Windowing... 58%\r",
      "Windowing... 59%\r",
      "Windowing... 60%\r",
      "Windowing... 60%\r",
      "Windowing... 61%\r",
      "Windowing... 61%\r",
      "Windowing... 62%\r",
      "Windowing... 62%\r",
      "Windowing... 63%\r",
      "Windowing... 64%\r",
      "Windowing... 64%\r",
      "Windowing... 65%\r",
      "Windowing... 65%\r",
      "Windowing... 66%\r",
      "Windowing... 67%\r",
      "Windowing... 67%\r",
      "Windowing... 68%\r",
      "Windowing... 68%\r",
      "Windowing... 69%\r",
      "Windowing... 70%\r",
      "Windowing... 70%\r",
      "Windowing... 71%\r",
      "Windowing... 71%\r",
      "Windowing... 72%\r",
      "Windowing... 73%\r",
      "Windowing... 73%\r",
      "Windowing... 74%\r",
      "Windowing... 74%\r",
      "Windowing... 75%\r",
      "Windowing... 76%\r",
      "Windowing... 76%\r",
      "Windowing... 77%\r",
      "Windowing... 77%\r",
      "Windowing... 78%\r",
      "Windowing... 79%\r",
      "Windowing... 79%\r",
      "Windowing... 80%\r",
      "Windowing... 80%\r",
      "Windowing... 81%\r",
      "Windowing... 82%\r",
      "Windowing... 82%\r",
      "Windowing... 83%\r",
      "Windowing... 83%\r",
      "Windowing... 84%\r",
      "Windowing... 85%\r",
      "Windowing... 85%\r",
      "Windowing... 86%\r",
      "Windowing... 86%\r",
      "Windowing... 87%\r",
      "Windowing... 88%\r",
      "Windowing... 88%\r",
      "Windowing... 89%\r",
      "Windowing... 89%\r",
      "Windowing... 90%\r",
      "Windowing... 90%\r",
      "Windowing... 91%\r",
      "Windowing... 92%\r",
      "Windowing... 92%\r",
      "Windowing... 93%\r",
      "Windowing... 93%\r",
      "Windowing... 94%\r",
      "Windowing... 95%\r",
      "Windowing... 95%\r",
      "Windowing... 96%\r",
      "Windowing... 96%\r",
      "Windowing... 97%\r",
      "Windowing... 98%\r",
      "Windowing... 98%\r",
      "Windowing... 99%\r",
      "Windowing... 99%\r",
      "Generating sim matrices 3%\r",
      "Generating sim matrices 19%\r",
      "Generating sim matrices 26%\r",
      "Generating sim matrices 27%\r",
      "Generating sim matrices 29%\r",
      "Generating sim matrices 30%\r",
      "Generating sim matrices 38%\r",
      "Generating sim matrices 40%\r",
      "Generating sim matrices 44%\r",
      "Generating sim matrices 58%\r",
      "Generating sim matrices 61%\r",
      "Generating sim matrices 69%\r",
      "Generating sim matrices 70%\r",
      "Generating sim matrices 72%\r",
      "Generating sim matrices 73%\r",
      "Generating sim matrices 80%\r",
      "Generating sim matrices 84%\r",
      "Generating sim matrices 86%\r",
      "Generating sim matrices 96%\r",
      "Generating sim matrices 100%\r",
      "Clustering...\r",
      "progress - 0.0% | cutoff - 0.01 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 2.0% | cutoff - 0.02 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 4.0% | cutoff - 0.03 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 6.0% | cutoff - 0.04 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 8.0% | cutoff - 0.05 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 10.0% | cutoff - 0.060000000000000005 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 12.0% | cutoff - 0.06999999999999999 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 14.0% | cutoff - 0.08 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 16.0% | cutoff - 0.09 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 18.0% | cutoff - 0.09999999999999999 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 20.0% | cutoff - 0.11 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 22.0% | cutoff - 0.12 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 24.0% | cutoff - 0.13 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 26.0% | cutoff - 0.14 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 28.0% | cutoff - 0.15000000000000002 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 30.0% | cutoff - 0.17 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 32.0% | cutoff - 0.2 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 34.0% | cutoff - 0.23 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 36.0% | cutoff - 0.26 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 38.0% | cutoff - 0.3 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 40.0% | cutoff - 0.34 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 42.0% | cutoff - 0.38 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 44.0% | cutoff - 0.42 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 46.0% | cutoff - 0.47000000000000003 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 48.0% | cutoff - 0.51 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 50.0% | cutoff - 0.56 | opt_cutoff - 0 | most_detections - 0\r",
      "progress - 52.0% | cutoff - 0.61 | opt_cutoff - 0.61 | most_detections - 1\r",
      "progress - 54.0% | cutoff - 0.67 | opt_cutoff - 0.61 | most_detections - 1\r",
      "progress - 56.0% | cutoff - 0.72 | opt_cutoff - 0.72 | most_detections - 3\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress - 58.0% | cutoff - 0.78 | opt_cutoff - 0.78 | most_detections - 4\r",
      "progress - 60.0% | cutoff - 0.83 | opt_cutoff - 0.78 | most_detections - 4\r",
      "progress - 62.0% | cutoff - 1.09 | opt_cutoff - 0.78 | most_detections - 4\r",
      "progress - 64.0% | cutoff - 1.37 | opt_cutoff - 0.78 | most_detections - 4\r",
      "progress - 66.0% | cutoff - 1.68 | opt_cutoff - 0.78 | most_detections - 4\r",
      "progress - 68.0% | cutoff - 2.0199999999999996 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 70.0% | cutoff - 2.3899999999999997 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 72.0% | cutoff - 2.78 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 74.0% | cutoff - 3.1999999999999997 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 76.0% | cutoff - 3.65 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 78.0% | cutoff - 4.12 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 80.0% | cutoff - 4.62 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 82.0% | cutoff - 5.14 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 84.0% | cutoff - 5.6899999999999995 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 86.0% | cutoff - 6.26 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 88.0% | cutoff - 6.85 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 90.0% | cutoff - 7.47 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 92.0% | cutoff - 8.12 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 94.0% | cutoff - 8.79 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 96.0% | cutoff - 9.48 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "progress - 98.0% | cutoff - 10.01 | opt_cutoff - 2.0199999999999996 | most_detections - 7\r",
      "Re-Clustering...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[29 11 158 7 218 62 120]'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[376], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m results_file:\n\u001b[0;32m     75\u001b[0m             json\u001b[38;5;241m.\u001b[39mdump(results, results_file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m results \u001b[38;5;241m=\u001b[39m main()\n",
      "Cell \u001b[1;32mIn[376], line 44\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m _, A_sparse, _, B_sparse, K_dense, K_sparse, stats \u001b[38;5;241m=\u001b[39m generate_synthetic_data(params)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(stats[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(A_sparse[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# if there are too many spikes, or no spikes, then don't bother scanning\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     pattern_template, sublist_keys_filt, pattern_img \u001b[38;5;241m=\u001b[39m scan_raster(A_sparse[\u001b[38;5;241m1\u001b[39m],A_sparse[\u001b[38;5;241m0\u001b[39m],window_dim\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pattern_img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m         pattern_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(pattern_img,axes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[358], line 63\u001b[0m, in \u001b[0;36mscan_raster\u001b[1;34m(T_labels, N_labels, window_dim)\u001b[0m\n\u001b[0;32m     61\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m sublist_keys_filt:\n\u001b[1;32m---> 63\u001b[0m     pattern_repetition_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(cluster_sq[\u001b[38;5;28mstr\u001b[39m(key)]),\u001b[38;5;28mlen\u001b[39m(clusters)))\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cluster_sq[\u001b[38;5;28mstr\u001b[39m(key)]):\n\u001b[0;32m     65\u001b[0m         pattern_repetition_labels[i][clusters\u001b[38;5;241m==\u001b[39mk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: '[29 11 158 7 218 62 120]'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Define the number of random samples you want to take\n",
    "    num_samples = 1  # Adjust this based on your computational resources\n",
    "\n",
    "    trials = 1\n",
    "\n",
    "    # List to hold the results\n",
    "    results = []\n",
    "\n",
    "    param_combinations = np.array(np.meshgrid(*scan_dict.values())).T.reshape(-1, len(scan_dict))\n",
    "    num_iterations = len(param_combinations)\n",
    "\n",
    "    # Generate random indices for sampling\n",
    "    random_indices = random.sample(range(num_iterations), num_samples) # this will not repeat indices! so I don't need to plan head!\n",
    "\n",
    "\n",
    "    # Iterate through parameter combinations\n",
    "    for idx in tqdm(random_indices):\n",
    "        for trial in range(0,trials):\n",
    "            seed=trial\n",
    "            performance_result = (np.nan, np.nan, np.nan)\n",
    "            SM_acc = np.array([])\n",
    "\n",
    "            if os.path.isfile(filename):\n",
    "                with open(filename, 'r') as results_file:\n",
    "                    results = json.load(results_file)\n",
    "\n",
    "            if idc in df['idc'].tolist():\n",
    "                while seed in df[df['idc'] == idc]['trial'].tolist():\n",
    "                    seed+=1\n",
    "\n",
    "\n",
    "\n",
    "            np.random.seed(seed)\n",
    "            params = {key: int(val) for key, val in zip(scan_dict.keys(), param_combinations[idx])}\n",
    "            params = {'M': 1, 'N': 100, 'D': 30, 'T': 1000, 'nrn_fr': 5, 'pg_fr': 10, 'background_noise_fr': 0}\n",
    "\n",
    "            # Run your program here to generate performance results\n",
    "            print(\"Params:\", params)\n",
    "            print(\"Generating raster plot...\")\n",
    "            start = time.time()\n",
    "            _, A_sparse, _, B_sparse, K_dense, K_sparse, stats = generate_synthetic_data(params)\n",
    "            if np.sum(stats[0]) <= 2000 or len(A_sparse[0])==0: # if there are too many spikes, or no spikes, then don't bother scanning\n",
    "                pattern_template, sublist_keys_filt, pattern_img = scan_raster(A_sparse[1],A_sparse[0],window_dim=params['D'])\n",
    "\n",
    "                if pattern_img is not None:\n",
    "                    pattern_img = np.transpose(pattern_img,axes=[1,2,0])\n",
    "                    SM_acc, _ = get_acc(K_dense, pattern_img)\n",
    "                    performance_result = (np.sum(SM_acc>0.8)/len(SM_acc), np.mean(SM_acc[SM_acc>0.8]), np.mean(SM_acc))\n",
    "                else:\n",
    "                    performance_result = (None, None, None)\n",
    "            end = time.time()\n",
    "\n",
    "            # Create a dictionary to store the result\n",
    "            result = {\n",
    "                'idc': idx,\n",
    "                'seed':seed,\n",
    "                'raster_fr':stats[0],\n",
    "                'pg_fr':stats[1],\n",
    "                'spikes_in_pg':stats[2],\n",
    "                **params,  # Unpack the parameters as separate columns\n",
    "                'SM_acc':SM_acc.tolist(),\n",
    "                'pct good SM detected':performance_result[0],\n",
    "                'good SM quality':performance_result[1],\n",
    "                'all SM quality':performance_result[2],\n",
    "                'time':round(end-start)\n",
    "            }\n",
    "\n",
    "            print(performance_result)\n",
    "\n",
    "            # Append the result to the list\n",
    "            results.append(result)\n",
    "        # Write the entire list of results to a JSON file\n",
    "        with open(filename, 'w') as results_file:\n",
    "            json.dump(results, results_file, indent=4)\n",
    "results = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "e499b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'M': 1, 'N': 100, 'D': 30, 'T': 1000, 'nrn_fr': 5, 'pg_fr': 10, 'background_noise_fr': 0}\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "2d2cdd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,A_sparse,_,_,_,_,_ = generate_synthetic_data(params)\n",
    "T_labels,N_labels = A_sparse[1],A_sparse[0]\n",
    "window_dim = params['D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "47512cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 Windows\n",
      "Re-Clustering... | cutoff - 10.01 | opt_cutoff - 2.0199999999999996 | most_detections - 8etections - 8\r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[26 4 244 136 83 73 14 28]'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[388], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m sublist_keys_filt:\n\u001b[1;32m---> 57\u001b[0m     pattern_repetition_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(cluster_sq[\u001b[38;5;28mstr\u001b[39m(key)]),\u001b[38;5;28mlen\u001b[39m(clusters)))\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cluster_sq[\u001b[38;5;28mstr\u001b[39m(key)]):\n\u001b[0;32m     59\u001b[0m         pattern_repetition_labels[i][clusters\u001b[38;5;241m==\u001b[39mk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: '[26 4 244 136 83 73 14 28]'"
     ]
    }
   ],
   "source": [
    "if window_dim == None:\n",
    "    window_dim = 100\n",
    "\n",
    "T_labels = np.round(T_labels).astype(int)\n",
    "T_labels, N_labels = np.unique(np.array([T_labels,N_labels]),axis=1) # This removes any spikes that occur at the same neuron at the same time\n",
    "N=max(N_labels)+1\n",
    "\n",
    "print(f'{len(T_labels)} Windows')\n",
    "windows = np.zeros((len(T_labels)),dtype='object')\n",
    "for i,window_time in enumerate(T_labels):\n",
    "    condition = (T_labels > window_time-window_dim) & (T_labels < window_time + window_dim)\n",
    "    window = np.array([T_labels[condition]-window_time, N_labels[condition]]).T\n",
    "    window =  {tuple(row) for row in  window}\n",
    "    windows[i] = window\n",
    "    print(f'Windowing... {round(100*i/len(T_labels))}%',end='\\r')\n",
    "\n",
    "\n",
    "max_iter = 50\n",
    "x = np.arange(0,100)\n",
    "fun = lambda x,c : x**(1+c)/10**(2*c+1)\n",
    "list_cutoffs = 0.01 + np.concatenate(((np.arange(0,100)/100)[:15], np.round(fun(x,.8),2)[10:25],np.round(fun(x,.8),2)[25::4],[10]))\n",
    "\n",
    "opt_cutoff = 0\n",
    "max_seq_rep = 0\n",
    "sim_mats = _get_sim_mats(windows, T_labels, N_labels)\n",
    "\n",
    "\n",
    "print(\"Clustering...\",end=\"\\r\")\n",
    "\n",
    "for i, cutoff in enumerate(list_cutoffs): \n",
    "    clusters = _cluster_windows(cutoff, N_labels, sim_mats)\n",
    "    cluster_sq, _sq_counts, sublist_keys_filt = _check_seq(clusters, T_labels, N_labels)\n",
    "\n",
    "    if len(sublist_keys_filt) != 0:\n",
    "        max_ = np.max([len(k) for k in sublist_keys_filt])\n",
    "        if max_seq_rep < max_:\n",
    "            max_seq_rep = max_\n",
    "            opt_cutoff=cutoff\n",
    "\n",
    "    print(f'progress - {100*i/max_iter}% | cutoff - {cutoff} | opt_cutoff - {opt_cutoff} | most_detections - {max_seq_rep}',end='\\r')\n",
    "\n",
    "clusters = _cluster_windows(opt_cutoff, N_labels, sim_mats)\n",
    "cluster_sq, sq_counts, sublist_keys_filt = _check_seq(clusters, T_labels, N_labels)\n",
    "\n",
    "\n",
    "''' to get the timings'''\n",
    "\n",
    "# Sort y according to x\n",
    "sorted_indices = np.argsort(T_labels)\n",
    "sorted_x = T_labels[sorted_indices]\n",
    "\n",
    "print(\"Re-Clustering...\",end=\"\\r\")\n",
    "\n",
    "all_times = []\n",
    "all_labels = []\n",
    "for key in sublist_keys_filt:\n",
    "    pattern_repetition_labels = np.zeros((len(cluster_sq[str(key)]),len(clusters)))\n",
    "    for i,k in enumerate(cluster_sq[str(key)]):\n",
    "        pattern_repetition_labels[i][clusters==k] = 1\n",
    "        pattern_repetition_labels[i] *= np.cumsum(pattern_repetition_labels[i])\n",
    "    pattern_repetition_labels = np.sum(pattern_repetition_labels,axis=0,dtype='int')\n",
    "    all_labels.append(pattern_repetition_labels)\n",
    "\n",
    "    sorted_y = pattern_repetition_labels[sorted_indices]\n",
    "    pattern_times = np.array([sorted_x[sorted_y==i][0] for i in range(1,max(pattern_repetition_labels)+1)])\n",
    "    all_times.append(pattern_times)\n",
    "\n",
    "print(\"Extracting templates...\",end=\"\\r\")\n",
    "\n",
    "pattern_template = []\n",
    "patterns = []\n",
    "for i in range(len(all_times)):\n",
    "    pattern = []\n",
    "    pattern_template.append([])\n",
    "    for time in all_times[i]:\n",
    "        condition = (T_labels > time-window_dim*2) & (T_labels < time + window_dim*2)\n",
    "        pattern = [tuple(k) for k in np.array([T_labels[condition]-time, N_labels[condition]]).T] # creating a list of tuples\n",
    "        pattern_template[-1] += pattern # adds all points of each pattern to template_pattern\n",
    "        patterns.append(pattern)\n",
    "    print(f\"Extracting templates... {round(100*i/len(all_times))}%\",end=\"\\r\")\n",
    "\n",
    "print(f\"{len(pattern_template)} patterns found...\")\n",
    "\n",
    "for i,pattern in enumerate(pattern_template):\n",
    "    counts = [pattern.count(k) for k in pattern]\n",
    "    pattern_template[i] = np.array(pattern)[np.where(counts == np.max(counts))[0]]\n",
    "    pattern_template[i][:,0] -= min(pattern_template[i][:,0])\n",
    "    pattern_template[i] = np.unique(pattern_template[i],axis=0)\n",
    "\n",
    "if len(pattern_template) == 0:\n",
    "    pass\n",
    "#     return pattern_template, sublist_keys_filt, None\n",
    "\n",
    "win_size = (K_dense.shape[0],1+max([max(k[:,0]) for k in pattern_template]))\n",
    "pattern_img = np.zeros((len(pattern_template),*win_size))\n",
    "for p,pattern in enumerate(pattern_template):\n",
    "    for (i,j) in pattern:\n",
    "        pattern_img[p,j,i] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "0125da33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0604d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "6f6c1d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  50,  80,  86,  89, 118, 124, 133, 138, 144, 148, 150, 155,\n",
       "        165, 180, 183, 209, 224],\n",
       "       [  3,   2,   3,   1,   2,   2,   3,   4,   0,   2,   3,   2,   3,\n",
       "          2,   3,   3,   1,   1]], dtype=int64)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_template[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "fc495ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 10, 16), (99, 10, 10))"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_dense.shape, pattern_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "00944780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idc</th>\n",
       "      <th>seed</th>\n",
       "      <th>raster_fr</th>\n",
       "      <th>pg_fr</th>\n",
       "      <th>spikes_in_pg</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>D</th>\n",
       "      <th>T</th>\n",
       "      <th>nrn_fr</th>\n",
       "      <th>background_noise_fr</th>\n",
       "      <th>SM_acc</th>\n",
       "      <th>pct good SM detected</th>\n",
       "      <th>good SM quality</th>\n",
       "      <th>all SM quality</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2438</td>\n",
       "      <td>0</td>\n",
       "      <td>[28, 58, 41, 57, 42, 22, 34, 39, 2, 32, 48, 43...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,...</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.125, 1.0, 1.0, 1.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5053</td>\n",
       "      <td>0</td>\n",
       "      <td>[22, 8, 18, 24, 10, 15, 27, 25, 13, 13, 19, 20...</td>\n",
       "      <td>4</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0, 0.2, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.170833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>411</td>\n",
       "      <td>0</td>\n",
       "      <td>[5, 1, 3, 15, 11, 7, 30, 11, 3, 13, 28, 12, 8,...</td>\n",
       "      <td>4</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,...</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.2, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1918</td>\n",
       "      <td>0</td>\n",
       "      <td>[840, 751, 866, 695, 735, 754, 657, 727, 713, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>[[2, 4, 2, 1, 0, 0, 0, 2, 0, 4, 5, 0, 0, 2, 1,...</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>1000</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.2, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3665</td>\n",
       "      <td>0</td>\n",
       "      <td>[10, 8, 8, 6, 6, 7, 6, 3, 5, 5, 3, 4, 3, 8, 2,...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 3, 0, 1,...</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>150</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.0, 0.2, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4445</td>\n",
       "      <td>0</td>\n",
       "      <td>[8, 19, 20, 25, 22, 8, 24, 31, 12, 18, 16, 23,...</td>\n",
       "      <td>8</td>\n",
       "      <td>[[0, 2, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 2, 0,...</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>70</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>[1.0, 0.08, 1.0, 0.08]</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4208</td>\n",
       "      <td>0</td>\n",
       "      <td>[74, 76, 101, 79, 101]</td>\n",
       "      <td>5</td>\n",
       "      <td>[[0, 2, 1, 2, 1], [0, 1, 1, 0, 2], [0, 1, 1, 0...</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.16666666666666666, 0.4, 0.2, 0.2, 0.0, 0.12...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.200174</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4713</td>\n",
       "      <td>0</td>\n",
       "      <td>[533, 703, 598, 614, 696, 655, 460, 466, 516, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>[[1, 2, 5, 7, 4, 3, 1, 3, 3, 2, 0, 1, 0, 3, 4,...</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>150</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.16666666666666666, 0.4, 0.2, 0.2, 0.0, 0.12...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1828</td>\n",
       "      <td>0</td>\n",
       "      <td>[104, 106, 150, 144, 48]</td>\n",
       "      <td>5</td>\n",
       "      <td>[[2, 3, 2, 3, 2], [1, 0, 0, 2, 2], [1, 2, 3, 5...</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1111111111111111, 0.09523809523809523, 0.66...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.153479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5199</td>\n",
       "      <td>0</td>\n",
       "      <td>[403, 353, 410, 281, 295, 351, 332, 309, 302, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[[2, 4, 2, 1, 0, 0, 0, 2, 0, 4, 5, 0, 0, 2, 1,...</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.1111111111111111, 0.09523809523809523, 0.66...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idc  seed                                          raster_fr  pg_fr  \\\n",
       "0  2438     0  [28, 58, 41, 57, 42, 22, 34, 39, 2, 32, 48, 43...      3   \n",
       "1  5053     0  [22, 8, 18, 24, 10, 15, 27, 25, 13, 13, 19, 20...      4   \n",
       "2   411     0  [5, 1, 3, 15, 11, 7, 30, 11, 3, 13, 28, 12, 8,...      4   \n",
       "3  1918     0  [840, 751, 866, 695, 735, 754, 657, 727, 713, ...      5   \n",
       "4  3665     0  [10, 8, 8, 6, 6, 7, 6, 3, 5, 5, 3, 4, 3, 8, 2,...      3   \n",
       "5  4445     0  [8, 19, 20, 25, 22, 8, 24, 31, 12, 18, 16, 23,...      8   \n",
       "6  4208     0                             [74, 76, 101, 79, 101]      5   \n",
       "7  4713     0  [533, 703, 598, 614, 696, 655, 460, 466, 516, ...     10   \n",
       "8  1828     0                           [104, 106, 150, 144, 48]      5   \n",
       "9  5199     0  [403, 353, 410, 281, 295, 351, 332, 309, 302, ...      4   \n",
       "\n",
       "                                        spikes_in_pg   M    N    D     T  \\\n",
       "0  [[0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,...  64   60   30  1000   \n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...  32   30   10  1000   \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,...  16  100   10  1000   \n",
       "3  [[2, 4, 2, 1, 0, 0, 0, 2, 0, 4, 5, 0, 0, 2, 1,...  64   60  150  1000   \n",
       "4  [[1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 3, 0, 1,...   4   30  150  1000   \n",
       "5  [[0, 2, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 2, 0,...   4   30   70  1000   \n",
       "6  [[0, 2, 1, 2, 1], [0, 1, 1, 0, 2], [0, 1, 1, 0...  16    5   70  1000   \n",
       "7  [[1, 2, 5, 7, 4, 3, 1, 3, 3, 2, 0, 1, 0, 3, 4,...  32   30  150  1000   \n",
       "8  [[2, 3, 2, 3, 2], [1, 0, 0, 2, 2], [1, 2, 3, 5...  16    5  150  1000   \n",
       "9  [[2, 4, 2, 1, 0, 0, 0, 2, 0, 4, 5, 0, 0, 2, 1,...  64  100  150  1000   \n",
       "\n",
       "   nrn_fr  background_noise_fr  \\\n",
       "0       5                    2   \n",
       "1       5                   10   \n",
       "2      15                    0   \n",
       "3      15                    1   \n",
       "4       5                    5   \n",
       "5      10                    5   \n",
       "6      10                    5   \n",
       "7      10                    5   \n",
       "8      10                    1   \n",
       "9      10                   10   \n",
       "\n",
       "                                              SM_acc  pct good SM detected  \\\n",
       "0                             [0.125, 1.0, 1.0, 1.0]                   NaN   \n",
       "1  [0.0, 0.2, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...               0.09375   \n",
       "2  [0.0, 0.2, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                   NaN   \n",
       "3  [0.0, 0.2, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                   NaN   \n",
       "4  [0.0, 0.2, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                   NaN   \n",
       "5                             [1.0, 0.08, 1.0, 0.08]               0.50000   \n",
       "6  [0.16666666666666666, 0.4, 0.2, 0.2, 0.0, 0.12...               0.00000   \n",
       "7  [0.16666666666666666, 0.4, 0.2, 0.2, 0.0, 0.12...                   NaN   \n",
       "8  [0.1111111111111111, 0.09523809523809523, 0.66...               0.00000   \n",
       "9  [0.1111111111111111, 0.09523809523809523, 0.66...                   NaN   \n",
       "\n",
       "   good SM quality  all SM quality  time  \n",
       "0              NaN             NaN     0  \n",
       "1              1.0        0.170833     1  \n",
       "2              NaN             NaN     0  \n",
       "3              NaN             NaN     0  \n",
       "4              NaN             NaN     1  \n",
       "5              1.0        0.540000     1  \n",
       "6              NaN        0.200174     1  \n",
       "7              NaN             NaN     0  \n",
       "8              NaN        0.153479     1  \n",
       "9              NaN             NaN     1  "
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile(filename):\n",
    "    with open(filename, 'r') as results_file:\n",
    "        test = json.load(results_file)\n",
    "[k['idc'] for k in test]\n",
    "df = pd.DataFrame(test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "996bd78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idc                                                                 1546\n",
      "seed                                                                   0\n",
      "raster_fr              [2, 8, 14, 13, 23, 30, 14, 27, 5, 5, 6, 6, 12,...\n",
      "pg_fr                                                                  4\n",
      "spikes_in_pg           [[0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,...\n",
      "M                                                                      4\n",
      "N                                                                     60\n",
      "D                                                                     30\n",
      "T                                                                   1000\n",
      "nrn_fr                                                                10\n",
      "background_noise_fr                                                    1\n",
      "performance                                                   [0.0, 0.0]\n",
      "time                                                                   2\n",
      "Name: 9, dtype: object\n",
      "843\n"
     ]
    }
   ],
   "source": [
    "i=9\n",
    "print(df.iloc[i])\n",
    "print(np.sum(df.iloc[i]['raster_fr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "2930d3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df[df['idc'] == 2925]['raster_fr'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6e8503e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4622]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['idc'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2dada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if idc in df['idc'].tolist():\n",
    "    while seed in df[df['idc'] == idc]['trial'].tolist():\n",
    "        seed+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d2392eb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'scan_statds.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[212], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscan_statds.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m results_file:\n\u001b[0;32m      2\u001b[0m     test \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(results_file)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mysnn\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scan_statds.json'"
     ]
    }
   ],
   "source": [
    "with open('scan_statds.json', 'r') as results_file:\n",
    "    test = json.load(results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ddf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd9e1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8319fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "        # Define the number of random samples you want to take\n",
    "    num_samples = 1  # Adjust this based on your computational resources\n",
    "    \n",
    "    trials = 1\n",
    "    \n",
    "    # List to hold the results\n",
    "    results = []\n",
    "    \n",
    "    param_combinations = np.array(np.meshgrid(*scan_dict.values())).T.reshape(-1, len(scan_dict))\n",
    "    num_iterations = len(param_combinations)\n",
    "    \n",
    "    # Generate random indices for sampling\n",
    "    random_indices = random.sample(range(num_iterations), num_samples)\n",
    "    \n",
    "    # Iterate through parameter combinations\n",
    "    for idx in tqdm(random_indices):\n",
    "        for trial in range(0,trials):\n",
    "            seed=trial\n",
    "            np.random.seed(seed)\n",
    "            params = {key: int(val) for key, val in zip(scan_dict.keys(), param_combinations[idx])}\n",
    "\n",
    "            # Run your program here to generate performance results\n",
    "            print(\"Params:\", params)\n",
    "            print(\"Generating raster plot...\")\n",
    "            _, A_sparse, _, B_sparse, K_dense, K_sparse, stats = generate_synthetic_data(params)\n",
    "            print(\"Clustering...\")\n",
    "            pattern_template, sublist_keys_filt, pattern_img = scan_raster(A_sparse[1],A_sparse[0],window_dim=params['D'])\n",
    "            if type(pattern_img) != np.ndarray:\n",
    "                performance_result = (0,0)\n",
    "            else:\n",
    "                pattern_img = np.transpose(pattern_img,axes=[1,2,0])\n",
    "                SM_acc, _ = get_acc(K_dense, pattern_img)\n",
    "                performance_result = (np.sum(SM_acc>0.8)/len(SM_acc), np.mean(SM_acc))\n",
    "\n",
    "            # Create a dictionary to store the result\n",
    "            result = {\n",
    "                'idc': idx,\n",
    "                'trial':trial,\n",
    "                'data':[A_sparse,K_sparse,B_sparse],\n",
    "                **params,  # Unpack the parameters as separate columns\n",
    "                'performance':performance_result\n",
    "            }\n",
    "            \n",
    "            print(performance_result)\n",
    "\n",
    "            # Append the result to the list\n",
    "            results.append(result)\n",
    "        # Write the entire list of results to a JSON file\n",
    "    with open('scan_stats.json', 'w') as results_file:\n",
    "        json.dump(results, results_file, indent=4)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "102d9b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model default parameters\n",
    "'''\n",
    "\n",
    "M = 4 # Number of Spiking motifs\n",
    "N = 20 # Number of input neurons\n",
    "D = 71 # temporal depth of receptive field\n",
    "T = 1000\n",
    "nrn_fr = 15 # hz\n",
    "pg_fr = 6 # hz\n",
    "background_noise_fr = 10 # hz\n",
    "seed=41\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "25860de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {\n",
    "    'M':M,\n",
    "    'N':N,\n",
    "    'D':D,\n",
    "    'T':T,\n",
    "    'nrn_fr':nrn_fr,\n",
    "    'pg_fr':pg_fr,\n",
    "    'background_noise_fr':background_noise_fr,\n",
    "    'seed':seed\n",
    "}\n",
    "scan_dict = {\n",
    "    'M':[1,4,16,32,64],\n",
    "    'N':[5,30,60,100],\n",
    "    'D':[10,30,70,150],\n",
    "    'T':[1000],\n",
    "    'nrn_fr':[5,10,15],\n",
    "    'pg_fr':[3,4,5,8,10],\n",
    "    'background_noise_fr':[0,1,2,5,10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5d2b7139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'M': 1, 'N': 60, 'D': 70, 'T': 1000, 'nrn_fr': 15, 'pg_fr': 5, 'background_noise_fr': 2}\n",
      "Generating raster plot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m main()\n",
      "Cell \u001b[1;32mIn[144], line 26\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams:\u001b[39m\u001b[38;5;124m\"\u001b[39m, params)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating raster plot...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m _, A_sparse, _, B_sparse, K_dense, K_sparse \u001b[38;5;241m=\u001b[39m generate_synthetic_data(params)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClustering...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m pattern_template, sublist_keys_filt, pattern_img \u001b[38;5;241m=\u001b[39m scan_raster(A_sparse[\u001b[38;5;241m1\u001b[39m],A_sparse[\u001b[38;5;241m0\u001b[39m],window_dim\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 6)"
     ]
    }
   ],
   "source": [
    "results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdd883fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'M': 32, 'N': 5, 'D': 150, 'T': 1000, 'nrn_fr': 5, 'pg_fr': 5, 'background_noise_fr': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfe4f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, A_sparse, _, B_sparse, K_dense, K_sparse = generate_synthetic_data(params)\n",
    "N_labels, T_labels = A_sparse[0], A_sparse[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03651dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_dim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b6679d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning spikes...\r",
      "Windowing... 717\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m opt_cutoff \u001b[38;5;241m=\u001b[39m cutoff\n\u001b[0;32m     27\u001b[0m max_seq_rep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 28\u001b[0m sim_mats \u001b[38;5;241m=\u001b[39m _get_sim_mats(windows, T_labels, N_labels)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m iter_ \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_iter: \u001b[38;5;66;03m# this is just a for loop...\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     clusters \u001b[38;5;241m=\u001b[39m _cluster_windows(cutoff, N_labels, sim_mats)\n",
      "Cell \u001b[1;32mIn[24], line 15\u001b[0m, in \u001b[0;36m_get_sim_mats\u001b[1;34m(windows, T_labels, N_labels)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 x[i,j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(common_rows)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(windows_n[i]),\u001b[38;5;28mlen\u001b[39m(windows_n[j]))\n\u001b[0;32m     14\u001b[0m         np\u001b[38;5;241m.\u001b[39mfill_diagonal(x,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;66;03m# make sure the diagonals are zero, this is important the more spikes there are...\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m         sim_mats[n] \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sim_mats\n",
      "\u001b[1;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Cleaning spikes...',end='\\r')\n",
    "if window_dim == None:\n",
    "    window_dim = 100\n",
    "\n",
    "T_labels = np.round(T_labels).astype(int)\n",
    "T_labels, N_labels = np.unique(np.array([T_labels,N_labels]),axis=1) # This removes any spikes that occur at the same neuron at the same time\n",
    "N=max(N_labels)+1\n",
    "\n",
    "print(f'Windowing... {len(T_labels)}')\n",
    "windows = np.zeros((len(T_labels)),dtype='object')\n",
    "for i,window_time in enumerate(T_labels):\n",
    "    condition = (T_labels > window_time-window_dim) & (T_labels < window_time + window_dim)\n",
    "    window = np.array([T_labels[condition]-window_time, N_labels[condition]]).T\n",
    "    window =  {tuple(row) for row in  window}\n",
    "    windows[i] = window\n",
    "\n",
    "\n",
    "# Set the cutoff value for clustering\n",
    "cutoff = 0\n",
    "lr = 0.01\n",
    "\n",
    "max_iter=50\n",
    "lr = 0.01\n",
    "iter_ = 0\n",
    "\n",
    "opt_cutoff = cutoff\n",
    "max_seq_rep = 0\n",
    "sim_mats = _get_sim_mats(windows, T_labels, N_labels)\n",
    "\n",
    "while iter_ <= max_iter: # this is just a for loop...\n",
    "    clusters = _cluster_windows(cutoff, N_labels, sim_mats)\n",
    "    cluster_sq, _sq_counts, sublist_keys_filt = _check_seq(clusters, T_labels, N_labels)\n",
    "\n",
    "    if len(sublist_keys_filt) != 0:\n",
    "        max_ = np.max([len(k) for k in sublist_keys_filt])\n",
    "        if max_seq_rep < max_:\n",
    "            max_seq_rep = max_\n",
    "            opt_cutoff=cutoff\n",
    "\n",
    "    cutoff += lr\n",
    "    iter_ +=1\n",
    "\n",
    "\n",
    "    print(f'iter - {iter_/max_iter} | cutoff - {cutoff} | opt_cutoff - {opt_cutoff} | most_detections - {max_seq_rep}',end='\\r')\n",
    "\n",
    "clusters = _cluster_windows(opt_cutoff, N_labels, sim_mats)\n",
    "cluster_sq, sq_counts, sublist_keys_filt = _check_seq(clusters, T_labels, N_labels)\n",
    "\n",
    "\n",
    "''' to get the timings'''\n",
    "\n",
    "# Sort y according to x\n",
    "sorted_indices = np.argsort(T_labels)\n",
    "sorted_x = T_labels[sorted_indices]\n",
    "\n",
    "all_times = []\n",
    "all_labels = []\n",
    "for key in sublist_keys_filt:\n",
    "    pattern_repetition_labels = np.zeros((len(cluster_sq[str(key)]),len(clusters)))\n",
    "    for i,k in enumerate(cluster_sq[str(key)]):\n",
    "        pattern_repetition_labels[i][clusters==k] = 1\n",
    "        pattern_repetition_labels[i] *= np.cumsum(pattern_repetition_labels[i])\n",
    "    pattern_repetition_labels = np.sum(pattern_repetition_labels,axis=0,dtype='int')\n",
    "    all_labels.append(pattern_repetition_labels)\n",
    "\n",
    "    sorted_y = pattern_repetition_labels[sorted_indices]\n",
    "    pattern_times = np.array([sorted_x[sorted_y==i][0] for i in range(1,max(pattern_repetition_labels)+1)])\n",
    "    all_times.append(pattern_times)\n",
    "\n",
    "pattern_template = []\n",
    "patterns = []\n",
    "for i in range(len(all_times)):\n",
    "    pattern = []\n",
    "    pattern_template.append([])\n",
    "    for time in all_times[i]:\n",
    "        condition = (T_labels > time-window_dim*2) & (T_labels < time + window_dim*2)\n",
    "        pattern = [tuple(k) for k in np.array([T_labels[condition]-time, N_labels[condition]]).T] # creating a list of tuples\n",
    "        pattern_template[-1] += pattern # adds all points of each pattern to template_pattern\n",
    "        patterns.append(pattern)\n",
    "\n",
    "for i,pattern in enumerate(pattern_template):\n",
    "    counts = [pattern.count(k) for k in pattern]\n",
    "    pattern_template[i] = np.array(pattern)[np.where(counts == np.max(counts))[0]]\n",
    "    pattern_template[i][:,0] -= min(pattern_template[i][:,0])\n",
    "    pattern_template[i] = np.unique(pattern_template[i],axis=0)\n",
    "\n",
    "if len(pattern_template) == 0:\n",
    "    return pattern_template, sublist_keys_filt, None\n",
    "\n",
    "win_size = (N,1+max([max(k[:,0]) for k in pattern_template]))\n",
    "pattern_img = np.zeros((len(pattern_template),*win_size))\n",
    "for p,pattern in enumerate(pattern_template):\n",
    "    for (i,j) in pattern:\n",
    "        pattern_img[p,j,i] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd735b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mats = np.zeros(np.max(N_labels)+1,dtype='object')\n",
    "for n in np.unique(N_labels):\n",
    "    idc = np.where(N_labels==n)[0]\n",
    "    windows_n = windows[idc]\n",
    "    if len(windows_n) > 1:\n",
    "        x = np.zeros((len(windows_n),len(windows_n)))\n",
    "        for i in range(windows_n.shape[0]):\n",
    "            for j in range(windows_n.shape[0]):\n",
    "                common_rows = windows_n[i].intersection(windows_n[j])\n",
    "                num_identical_rows = len(common_rows)\n",
    "                x[i,j] = len(common_rows)/min(len(windows_n[i]),len(windows_n[j]))\n",
    "        np.fill_diagonal(x,0)# make sure the diagonals are zero, this is important the more spikes there are...\n",
    "        sim_mats[n] = x-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08a9b4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, 1, 1, 2, 1, 2, 1, 1, 3, 1, 4, 0, 2, 1, 3, 4, 3, 0, 4, 0,\n",
       "       2, 2, 3, 0, 2, 2, 3, 4, 1, 3, 0, 4, 4, 2, 0, 4, 4, 3, 4, 1, 3, 1,\n",
       "       4, 3, 4, 1, 3, 0, 1, 0, 2, 2, 4, 2, 4, 2, 3, 1, 3, 4, 1, 2, 2, 3,\n",
       "       2, 1, 2, 4, 4, 0, 1, 3, 1, 2, 0, 4, 0, 1, 1, 0, 1, 2, 3, 1, 4, 3,\n",
       "       0, 2, 4, 0, 1, 0, 4, 0, 1, 3, 2, 4, 0, 3, 1, 2, 3, 1, 0, 3, 3, 0,\n",
       "       3, 4, 0, 0, 0, 2, 3, 0, 4, 0, 1, 3, 3, 3, 2, 3, 4, 0, 3, 1, 3, 3,\n",
       "       0, 2, 0, 0, 2, 1, 2, 0, 2, 1, 2, 4, 1, 3, 4, 0, 3, 4, 1, 1, 1, 2,\n",
       "       0, 2, 2, 0, 0, 1, 3, 0, 2, 2, 3, 3, 0, 1, 3, 3, 0, 1, 4, 3, 3, 4,\n",
       "       0, 1, 3, 3, 0, 3, 0, 1, 2, 1, 2, 3, 4, 1, 0, 0, 1, 2, 4, 0, 3, 1,\n",
       "       0, 3, 2, 3, 4, 3, 1, 4, 1, 2, 1, 0, 4, 3, 0, 3, 1, 2, 0, 3, 0, 2,\n",
       "       2, 2, 2, 1, 3, 4, 1, 0, 4, 2, 0, 1, 4, 4, 0, 2, 1, 1, 2, 3, 1, 3,\n",
       "       0, 3, 0, 0, 1, 1, 2, 2, 3, 4, 1, 3, 2, 1, 4, 0, 1, 0, 4, 2, 3, 2,\n",
       "       1, 4, 3, 0, 1, 0, 0, 3, 0, 2, 1, 1, 1, 2, 0, 4, 1, 0, 2, 0, 1, 3,\n",
       "       0, 1, 0, 2, 3, 1, 0, 2, 4, 0, 3, 1, 3, 0, 4, 0, 3, 4, 0, 4, 2, 1,\n",
       "       1, 3, 3, 2, 1, 0, 1, 2, 3, 3, 3, 4, 1, 0, 0, 2, 3, 2, 2, 0, 4, 2,\n",
       "       0, 1, 2, 3, 4, 2, 4, 4, 0, 1, 3, 1, 4, 0, 1, 2, 3, 4, 1, 3, 1, 0,\n",
       "       3, 4, 0, 2, 0, 2, 4, 1, 0, 3, 4, 4, 3, 3, 0, 2, 3, 3, 3, 0, 2, 2,\n",
       "       0, 3, 2, 1, 1, 4, 2, 0, 1, 0, 1, 1, 2, 2, 3, 3, 1, 0, 1, 2, 4, 1,\n",
       "       2, 3, 4, 1, 2, 3, 4, 1, 1, 3, 1, 1, 2, 1, 2, 3, 3, 0, 4, 1, 3, 2,\n",
       "       0, 3, 1, 2, 1, 3, 3, 2, 0, 3, 3, 2, 3, 4, 3, 1, 0, 4, 3, 0, 0, 1,\n",
       "       0, 4, 3, 0, 3, 2, 2, 3, 3, 0, 3, 4, 0, 1, 0, 2, 3, 4, 2, 0, 2, 3,\n",
       "       1, 3, 0, 3, 1, 3, 2, 3, 2, 3, 3, 3, 0, 1, 3, 1, 0, 3, 2, 3, 3, 1,\n",
       "       2, 3, 1, 0, 2, 0, 4, 1, 1, 3, 0, 3, 4, 0, 1, 1, 0, 2, 0, 1, 3, 3,\n",
       "       1, 0, 1, 2, 3, 2, 1, 0, 2, 3, 0, 1, 2, 3, 2, 4, 4, 2, 3, 0, 1, 2,\n",
       "       3, 2, 0, 2, 0, 3, 1, 1, 1, 4, 2, 4, 2, 1, 1, 3, 3, 3, 4, 0, 0, 2,\n",
       "       0, 2, 3, 3, 1, 1, 3, 4, 2, 0, 3, 0, 1, 2, 4, 1, 1, 2, 1, 2, 3, 2,\n",
       "       1, 3, 0, 4, 4, 0, 3, 4, 0, 1, 2, 4, 1, 2, 0, 4, 0, 1, 4, 1, 3, 0,\n",
       "       2, 4, 1, 0, 1, 4, 1, 4, 0, 1, 3, 3, 1, 4, 3, 4, 0, 3, 0, 1, 3, 0,\n",
       "       1, 0, 3, 3, 3, 3, 4, 1, 0, 1, 3, 4, 4, 2, 3, 0, 3, 4, 4, 3, 1, 4,\n",
       "       1, 4, 1, 3, 4, 4, 0, 0, 2, 2, 3, 3, 3, 4, 1, 3, 0, 4, 0, 2, 3, 2,\n",
       "       1, 1, 2, 4, 0, 0, 4, 2, 1, 0, 1, 3, 4, 1, 3, 1, 0, 3, 4, 0, 0, 1,\n",
       "       2, 0, 2, 0, 2, 1, 2, 3, 1, 0, 2, 3, 4, 0, 2, 3, 1, 4, 0, 1, 2, 2,\n",
       "       2, 1, 4, 1, 0, 2, 0, 0, 4, 1, 3, 4, 1], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd6a5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_sim_mats(windows, T_labels, N_labels):\n",
    "    sim_mats = np.zeros(np.max(N_labels),dtype='object')\n",
    "    for n in np.unique(N_labels):\n",
    "        idc = np.where(N_labels==n)[0]\n",
    "        windows_n = windows[idc]\n",
    "        if len(windows_n) > 1:\n",
    "            x = np.zeros((len(windows_n),len(windows_n)))\n",
    "            for i in range(windows_n.shape[0]):\n",
    "                for j in range(windows_n.shape[0]):\n",
    "                    common_rows = windows_n[i].intersection(windows_n[j])\n",
    "                    num_identical_rows = len(common_rows)\n",
    "                    x[i,j] = len(common_rows)/min(len(windows_n[i]),len(windows_n[j]))\n",
    "            np.fill_diagonal(x,0)# make sure the diagonals are zero, this is important the more spikes there are...\n",
    "            sim_mats[n] = x-1 \n",
    "    return sim_mats\n",
    "\n",
    "def _cluster_windows(cutoff, N_labels, sim_mats):\n",
    "    clusters = np.zeros_like(N_labels)\n",
    "    for n in np.unique(N_labels):\n",
    "        idc = np.where(N_labels==n)[0]\n",
    "        if (type(sim_mats[n]) == np.ndarray) and (not np.all(sim_mats[n] == 0)):\n",
    "            l = max(clusters)+1\n",
    "            clusters[idc]= l+fcluster(linkage(sim_mats[n], method='complete'), cutoff, criterion='distance')\n",
    "    return clusters\n",
    "\n",
    "def _check_seq(clusters, T_labels, N_labels):\n",
    "\n",
    "    time_differences = []\n",
    "    cluster_sq = {}\n",
    "    for cluster in np.unique(clusters):\n",
    "        temp = list(np.diff(np.unique(T_labels[clusters == cluster])))\n",
    "        str_temp = str(temp)\n",
    "        time_differences.append(temp)\n",
    "        if str_temp in cluster_sq.keys():\n",
    "            cluster_sq[str_temp] = cluster_sq[str_temp] + [cluster]\n",
    "        else:\n",
    "            cluster_sq[str_temp] = [cluster]\n",
    "\n",
    "    # Convert the list of lists to a set of tuples to remove duplicates\n",
    "    unique_sublists_set = set(tuple(sublist) for sublist in time_differences if sublist)\n",
    "\n",
    "    # Convert the set of tuples back to a list of lists\n",
    "    unique_sublists = [list(sublist) for sublist in unique_sublists_set]\n",
    "\n",
    "    # Count the occurrences of each unique sublist in the original list\n",
    "    sublist_counts = Counter(tuple(sublist) for sublist in time_differences if sublist)\n",
    "\n",
    "    # Print the unique sublists and their respective counts\n",
    "    sq_counts = np.zeros(len(sublist_counts)) \n",
    "    for i,sublist in enumerate(unique_sublists):\n",
    "        count = sublist_counts[tuple(sublist)]\n",
    "        sq_counts[i] = count\n",
    "    #     print(f\"{sublist}: {count} occurrences\")\n",
    "    sublist_keys_np = np.array([list(key) for key in sublist_counts.keys()],dtype='object')\n",
    "    sublist_keys_filt = sublist_keys_np[np.array(list(sublist_counts.values())) >1] # only bother clustering repetitions that appear for more than one neuron\n",
    "\n",
    "    return cluster_sq, sq_counts, sublist_keys_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef52a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
